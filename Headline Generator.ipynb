{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf5b65e-8643-4258-b361-11ce16621f24",
   "metadata": {},
   "source": [
    "## Headline Generator\n",
    "\n",
    "Purpose: to train a model to predict words in a headline, and use that model to create headlines of various lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77065c02-5505-4b50-9738-df9e2e4c1ff5",
   "metadata": {},
   "source": [
    "### Reading in the Data\n",
    "The dataset consists of headlines from the New York Times newspaper over the course of several months. First, read in all the headlines from the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2656901-7e05-4ea7-b958-71d126ded7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"./nyt-comments\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "import opendatasets as od\n",
    "dataset = 'https://www.kaggle.com/datasets/aashita/nyt-comments'\n",
    "od.download(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9664c71a-0867-453d-a2b3-eb2f874382d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ArticlesFeb2017.csv',\n",
       " 'CommentsFeb2018.csv',\n",
       " 'ArticlesApril2017.csv',\n",
       " 'CommentsApril2018.csv',\n",
       " 'ArticlesMarch2018.csv',\n",
       " 'CommentsMarch2017.csv',\n",
       " 'ArticlesMay2017.csv',\n",
       " 'ArticlesJan2017.csv',\n",
       " 'CommentsJan2018.csv',\n",
       " 'CommentsMarch2018.csv',\n",
       " 'ArticlesJan2018.csv',\n",
       " 'CommentsMay2017.csv',\n",
       " 'CommentsJan2017.csv',\n",
       " 'ArticlesMarch2017.csv',\n",
       " 'CommentsApril2017.csv',\n",
       " 'ArticlesFeb2018.csv',\n",
       " 'CommentsFeb2017.csv',\n",
       " 'ArticlesApril2018.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "nyt_dir = './data/nyt-comments'\n",
    "os.listdir(nyt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeca8c27-53b4-4fdd-844f-27b01a1da49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9335"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_headlines = []  # list retainer for headlines\n",
    "\n",
    "for filename in os.listdir(nyt_dir):\n",
    "    if 'Articles' in filename:\n",
    "        # read in all relevant data from the CSV file\n",
    "        headlines_df = pd.read_csv(nyt_dir + '/' + filename)\n",
    "        # add all headlines to the list\n",
    "        all_headlines.extend(list(headlines_df.headline.values))\n",
    "\n",
    "len(all_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a59740c-1d12-4fa4-b2cf-5c3b02b3f124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N.F.L. vs. Politics Has Been Battle All Season Long',\n",
       " 'Voice. Vice. Veracity.',\n",
       " 'A Stand-Up’s Downward Slide',\n",
       " 'New York Today: A Groundhog Has Her Day',\n",
       " 'A Swimmer’s Communion With the Ocean',\n",
       " 'Trail Activity',\n",
       " 'Super Bowl',\n",
       " 'Trump’s Mexican Shakedown',\n",
       " 'Pence’s Presidential Pet',\n",
       " 'Fruit of a Poison Tree',\n",
       " 'The Peculiar Populism of Donald Trump',\n",
       " 'Questions for: ‘On Alaska’s Coldest Days, a Village Draws Close for Warmth’',\n",
       " 'The New Kids',\n",
       " 'What My Chinese Mother Made',\n",
       " 'Do You Think Teenagers Can Make a Difference in the World?',\n",
       " 'Unknown',\n",
       " 'President Pledges to Let Politics Return to Pulpits',\n",
       " 'The Police Killed My Unarmed Son in 2012. I’m Still Waiting for Justice.',\n",
       " 'Video of Sheep Slaughtering Ignites a Dispute',\n",
       " 'This Will Change Your Mind']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_headlines[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc98f435-bd41-4d9f-a01e-72795586698f",
   "metadata": {},
   "source": [
    "### Cleaning the Data\n",
    "Since we are dealing with a natural language processing (NLP) task, we need to process the text in a way that computers can understand it with tokenization - representing each word with a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef3ad446-cf8f-4790-986f-9f6bfd63eb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8603"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove all headlines with the value of \"Unknown\"\n",
    "all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
    "len(all_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f435215f-2275-4092-b4f1-f35fc5e580c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N.F.L. vs. Politics Has Been Battle All Season Long',\n",
       " 'Voice. Vice. Veracity.',\n",
       " 'A Stand-Up’s Downward Slide',\n",
       " 'New York Today: A Groundhog Has Her Day',\n",
       " 'A Swimmer’s Communion With the Ocean',\n",
       " 'Trail Activity',\n",
       " 'Super Bowl',\n",
       " 'Trump’s Mexican Shakedown',\n",
       " 'Pence’s Presidential Pet',\n",
       " 'Fruit of a Poison Tree',\n",
       " 'The Peculiar Populism of Donald Trump',\n",
       " 'Questions for: ‘On Alaska’s Coldest Days, a Village Draws Close for Warmth’',\n",
       " 'The New Kids',\n",
       " 'What My Chinese Mother Made',\n",
       " 'Do You Think Teenagers Can Make a Difference in the World?',\n",
       " 'President Pledges to Let Politics Return to Pulpits',\n",
       " 'The Police Killed My Unarmed Son in 2012. I’m Still Waiting for Justice.',\n",
       " 'Video of Sheep Slaughtering Ignites a Dispute',\n",
       " 'This Will Change Your Mind',\n",
       " 'Busy Start for a President, and That Was in 1933']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_headlines[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5f5431-9a77-4e52-87d1-e344796e3a36",
   "metadata": {},
   "source": [
    "\n",
    "We also want remove punctuation and make our sentences lowercase. This will decrease the number of unique words/tokens, and the model will be easier to train. Filtering the sentences for punctuation and uppercase letters can be done through Keras Tokenizer.\n",
    "\n",
    "### Tokenization\n",
    "With tokenization, a piece of text is separated into smaller chunks - in this case, words. Each unique word is assigned a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6017fd2f-a2a7-4be4-abf1-2cba80ae51e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words:  11753\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# tokenize the words in the headlines\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_headlines)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print('Total words: ', total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6e2eba-0b41-4dcb-9152-1bdf499d4332",
   "metadata": {},
   "source": [
    "Can use word_index dictionary to see how the tokenizer saves the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9c20aa4-18f1-466a-8c07-ddadc9af532b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 2, 'plan': 82, 'man': 139, 'panama': 2931, 'canal': 5487}\n"
     ]
    }
   ],
   "source": [
    "# print a subset of the word_index dictionary created by Tokenizer\n",
    "subset_dict = {key: value for key, value in tokenizer.word_index.items() \\\n",
    "                if key in ['a', 'man', 'a', 'plan', 'a', 'canal', 'panama']}\n",
    "print(subset_dict)  # print a subset to visualize the tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e03d25-d6e1-464d-a930-09cc91f9d5cc",
   "metadata": {},
   "source": [
    "Can use texts_to_sequences method to see how the tokenizer saves the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "159efd8a-639e-46de-b1da-777bbce15787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2], [139], [2], [82], [2], [5487], [2931]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['a', 'man', 'a', 'plan', 'a', 'canal', 'panama'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cd9d04-31d0-4189-984c-303080c56a03",
   "metadata": {},
   "source": [
    "### Creating Sequences\n",
    "Now that the data is tokenized, we will need to create a sequence of tokens from the headlines. These sequences will be used to train the deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fdd9303-d272-47cd-94a1-22b14ead1320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n f', 'n f l', 'n f l vs', 'n f l vs politics', 'n f l vs politics has']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[193, 125],\n",
       " [193, 125, 253],\n",
       " [193, 125, 253, 157],\n",
       " [193, 125, 253, 157, 226],\n",
       " [193, 125, 253, 157, 226, 83]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert data to sequence of tokens\n",
    "input_sequences = []\n",
    "for line in all_headlines:\n",
    "    # convert the headline into a sequence of tokens\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\n",
    "    # create a series of sequences for each headline\n",
    "    for i in range(1, len(token_list)):\n",
    "        partial_sequence = token_list[:i+1]\n",
    "        input_sequences.append(partial_sequence)\n",
    "\n",
    "print(tokenizer.sequences_to_texts(input_sequences[:5]))\n",
    "input_sequences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45167adf-ff2c-4c88-8aa8-1987d75bdd54",
   "metadata": {},
   "source": [
    "### Padding Sequences\n",
    "These sequences are of various lengths. For the model to be able to train on the data, all the sequences need to be the same length. This can be done by adding padding to the sequences with Keras pad_sequences method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "087fffed-83a2-4afd-8d69-6b927b179adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       193, 125], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# determine max sequence length\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "\n",
    "# pad all sequences with zeros at the beginning to make them all max length\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "input_sequences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4db32f-9a7c-47c9-a74e-dd11b27458a0",
   "metadata": {},
   "source": [
    "### Creating Predictors and Target\n",
    "We need to split up the sequences into predictors and a target. The last word of the sequence will be the target, and the first words of the sequence will be the predictors. For example, in \"nvidia releases ampere graphics cards\", the predictors are \"nvidia releases ampere graphics\" and the target/label is \"cards\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef1594eb-e059-452e-ab9c-c49901feeb1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([125, 253, 157, 226,  83], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictors are every word except the last\n",
    "predictors = input_sequences[:,:-1]\n",
    "# labels are the last word\n",
    "labels = input_sequences[:,-1]\n",
    "labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7312533b-d624-48a5-9bd5-46398a26bade",
   "metadata": {},
   "source": [
    "The targets are categorical. We are predicting one word out of our possible total vocabulary. Instead of the model predicting scalar numbers, we will have it predict binary categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9169adad-7396-404d-a35d-a77cd1c775e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import utils\n",
    "\n",
    "labels = utils.to_categorical(labels, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1684fb0-624a-4743-9ae5-196dd768d895",
   "metadata": {},
   "source": [
    "### Creating the Model\n",
    "Embedding Layer: This layer will take the tokenized sequences and will learn an embedding for all of the words in the training dataset. The goal of this is to reduce the number of dimensions of the features. In this case, it will represent each word as a vector, and the information within that vector will contain the relationships between each word.\n",
    "\n",
    "Long Short Term Memory (LSTM) Layer: This is a type of recurrent neural network (RNN). While traditional RNNs allow information from more recent layers to contribute more than information from further back, LSTMs are a type of recurrent layer that are able to learn and retain longer term information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b2787d4-2456-4388-850c-22caa38a5a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sundar/Desktop/jupyter_practice/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:86: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# input is max length -1, as we've removed the last word for the label\n",
    "input_len = max_sequence_len - 1\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# add input embedding layer\n",
    "model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "\n",
    "# add LSTM layer with 100 units\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# add output layer\n",
    "model.add(Dense(total_words, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1f3ddf0-eb6f-4fb6-8b51-4a76393f1edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb3bc6e-3e07-4c0f-b7cc-c928e202e2a5",
   "metadata": {},
   "source": [
    "### Compiling and Training the Model\n",
    "As we are categorically predicting one word from our total vocab, we will choose categorical crossentropy. Note: we do not choose accuracy as a metric because text prediction is not measured as being more or less accurate in the same way as image classification.\n",
    "\n",
    "We will set the optimizer as a Adam optimizer, which is well suited for LSTM tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a842fa46-f933-466e-9933-6bde3de4102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aedab123-c4ab-440f-a619-355f993e611d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1666/1666\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 12ms/step - loss: 8.0648\n",
      "Epoch 2/10\n",
      "\u001b[1m1666/1666\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - loss: 7.4853\n",
      "Epoch 3/10\n",
      "\u001b[1m1666/1666\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 10ms/step - loss: 7.2626\n",
      "Epoch 4/10\n",
      "\u001b[1m1666/1666\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - loss: 7.0074\n",
      "Epoch 5/10\n",
      "\u001b[1m1666/1666\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - loss: 6.7484\n",
      "Epoch 6/10\n",
      "\u001b[1m1666/1666\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - loss: 6.4922\n",
      "Epoch 7/10\n",
      "\u001b[1m1666/1666\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - loss: 6.2163\n",
      "Epoch 8/10\n",
      "\u001b[1m1666/1666\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - loss: 5.9977\n",
      "Epoch 9/10\n",
      "\u001b[1m1666/1666\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - loss: 5.7629\n",
      "Epoch 10/10\n",
      "\u001b[1m1666/1666\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - loss: 5.5628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2b395f7a0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(predictors, labels, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb66d3a-fe49-43e6-8554-a235dbd4abd6",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "To make predictions, we need to start with a seed text and prepare it in the same way that we prepared our dataset (tokenizing and padding). We can create a function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "465fd259-5429-43f9-801a-415595b11052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_token(seed_text):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]  # convert to tokens\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')  # pad the lists of tokens before entering into model\n",
    "    prediction = model.predict(token_list, verbose=0)  # run the input through the model for prediction\n",
    "    predicted_classes = np.argmax(prediction, axis=1)  # for multi-class classification\n",
    "    return predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47e82f02-9797-4f52-a0c4-fd7fc2b54295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = predict_next_token(\"today in new york\") \n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a293b13-1b20-4d7c-a07d-f28d352ccad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e64992a-a2db-4858-8117-06f41de879f1",
   "metadata": {},
   "source": [
    "### Generate New Headlines\n",
    "With a function that helps with predicting new words from seed texts, we will create a function that can predict headlines of more than just one word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21cb7faa-4987-4b00-9cb8-ecb4cefb0e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_headline(seed_text, next_words=1):\n",
    "    for _ in range(next_words):\n",
    "        # predict next token\n",
    "        prediction = predict_next_token(seed_text)\n",
    "        # convert token to word\n",
    "        next_word = tokenizer.sequences_to_texts([prediction])[0]\n",
    "        # add next word to the headline. This headline will be used in the next pass of the loop.\n",
    "        seed_text += \" \" + next_word\n",
    "    # return headline as title-case\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a1a24637-92c9-4c69-a3f8-6915a20db5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Washington Dc Is A New York Of A\n",
      "Today In New York And The City Of A\n",
      "The School District Has The New York Times For\n",
      "Crime Has Become A World On The City\n"
     ]
    }
   ],
   "source": [
    "seed_texts = [\n",
    "    'washington dc is',\n",
    "    'today in new york',\n",
    "    'the school district has',\n",
    "    'crime has become']\n",
    "for seed in seed_texts:\n",
    "    print(generate_headline(seed, next_words=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db082f9e-5d52-4331-8049-6bdbc43860c6",
   "metadata": {},
   "source": [
    "The results are underwhelming after 10 epochs of training. The headlines make some kind of grammatical sense, but don't show a good contextual understanding. The results could improve by running more epochs. \n",
    "\n",
    "We could also improve the results by using pre-trained embeddings with Word2Vec or GloVe, rather than learning them during training with the Keras Embedding layer.\n",
    "\n",
    "It's important to note that NLP has moved beyond simple LSTM models to Transformer-based pre-trained models, which are able to learn language context from huge amounts of textual data. These pre-trained models are then used as a starting point for transfer learning to solve NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29c73bd-661d-4b4f-afc1-702056207d57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
