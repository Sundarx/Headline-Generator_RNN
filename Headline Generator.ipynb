{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf5b65e-8643-4258-b361-11ce16621f24",
   "metadata": {},
   "source": [
    "## Headline Generator\n",
    "\n",
    "Purpose: to train a model to predict words in a headline, and use that model to create headlines of various lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77065c02-5505-4b50-9738-df9e2e4c1ff5",
   "metadata": {},
   "source": [
    "### Reading in the Data\n",
    "The dataset consists of headlines from the New York Times newspaper over the course of several months. First, read in all the headlines from the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2656901-7e05-4ea7-b958-71d126ded7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/aashita/nyt-comments\n",
      "Downloading nyt-comments.zip to ./nyt-comments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 480M/480M [00:19<00:00, 26.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import opendatasets as od\n",
    "dataset = 'https://www.kaggle.com/datasets/aashita/nyt-comments'\n",
    "od.download(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9664c71a-0867-453d-a2b3-eb2f874382d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ArticlesFeb2017.csv',\n",
       " 'CommentsFeb2018.csv',\n",
       " 'ArticlesApril2017.csv',\n",
       " 'CommentsApril2018.csv',\n",
       " 'ArticlesMarch2018.csv',\n",
       " 'CommentsMarch2017.csv',\n",
       " 'ArticlesMay2017.csv',\n",
       " 'ArticlesJan2017.csv',\n",
       " 'CommentsJan2018.csv',\n",
       " 'CommentsMarch2018.csv',\n",
       " 'ArticlesJan2018.csv',\n",
       " 'CommentsMay2017.csv',\n",
       " 'CommentsJan2017.csv',\n",
       " 'ArticlesMarch2017.csv',\n",
       " 'CommentsApril2017.csv',\n",
       " 'ArticlesFeb2018.csv',\n",
       " 'CommentsFeb2017.csv',\n",
       " 'ArticlesApril2018.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "nyt_dir = './data/nyt-comments'\n",
    "os.listdir(nyt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeca8c27-53b4-4fdd-844f-27b01a1da49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9335"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_headlines = []  # list retainer for headlines\n",
    "\n",
    "for filename in os.listdir(nyt_dir):\n",
    "    if 'Articles' in filename:\n",
    "        # read in all relevant data from the CSV file\n",
    "        headlines_df = pd.read_csv(nyt_dir + '/' + filename)\n",
    "        # add all headlines to the list\n",
    "        all_headlines.extend(list(headlines_df.headline.values))\n",
    "\n",
    "len(all_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a59740c-1d12-4fa4-b2cf-5c3b02b3f124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N.F.L. vs. Politics Has Been Battle All Season Long',\n",
       " 'Voice. Vice. Veracity.',\n",
       " 'A Stand-Up’s Downward Slide',\n",
       " 'New York Today: A Groundhog Has Her Day',\n",
       " 'A Swimmer’s Communion With the Ocean',\n",
       " 'Trail Activity',\n",
       " 'Super Bowl',\n",
       " 'Trump’s Mexican Shakedown',\n",
       " 'Pence’s Presidential Pet',\n",
       " 'Fruit of a Poison Tree',\n",
       " 'The Peculiar Populism of Donald Trump',\n",
       " 'Questions for: ‘On Alaska’s Coldest Days, a Village Draws Close for Warmth’',\n",
       " 'The New Kids',\n",
       " 'What My Chinese Mother Made',\n",
       " 'Do You Think Teenagers Can Make a Difference in the World?',\n",
       " 'Unknown',\n",
       " 'President Pledges to Let Politics Return to Pulpits',\n",
       " 'The Police Killed My Unarmed Son in 2012. I’m Still Waiting for Justice.',\n",
       " 'Video of Sheep Slaughtering Ignites a Dispute',\n",
       " 'This Will Change Your Mind']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_headlines[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc98f435-bd41-4d9f-a01e-72795586698f",
   "metadata": {},
   "source": [
    "### Cleaning the Data\n",
    "Since we are dealing with a natural language processing (NLP) task, we need to process the text in a way that computers can understand it with tokenization - representing each word with a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef3ad446-cf8f-4790-986f-9f6bfd63eb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8603"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove all headlines with the value of \"Unknown\"\n",
    "all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
    "len(all_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f435215f-2275-4092-b4f1-f35fc5e580c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N.F.L. vs. Politics Has Been Battle All Season Long',\n",
       " 'Voice. Vice. Veracity.',\n",
       " 'A Stand-Up’s Downward Slide',\n",
       " 'New York Today: A Groundhog Has Her Day',\n",
       " 'A Swimmer’s Communion With the Ocean',\n",
       " 'Trail Activity',\n",
       " 'Super Bowl',\n",
       " 'Trump’s Mexican Shakedown',\n",
       " 'Pence’s Presidential Pet',\n",
       " 'Fruit of a Poison Tree',\n",
       " 'The Peculiar Populism of Donald Trump',\n",
       " 'Questions for: ‘On Alaska’s Coldest Days, a Village Draws Close for Warmth’',\n",
       " 'The New Kids',\n",
       " 'What My Chinese Mother Made',\n",
       " 'Do You Think Teenagers Can Make a Difference in the World?',\n",
       " 'President Pledges to Let Politics Return to Pulpits',\n",
       " 'The Police Killed My Unarmed Son in 2012. I’m Still Waiting for Justice.',\n",
       " 'Video of Sheep Slaughtering Ignites a Dispute',\n",
       " 'This Will Change Your Mind',\n",
       " 'Busy Start for a President, and That Was in 1933']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_headlines[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5f5431-9a77-4e52-87d1-e344796e3a36",
   "metadata": {},
   "source": [
    "\n",
    "We also want remove punctuation and make our sentences lowercase. This will decrease the number of unique words/tokens, and the model will be easier to train. Filtering the sentences for punctuation and uppercase letters can be done through Keras Tokenizer.\n",
    "\n",
    "### Tokenization\n",
    "With tokenization, a piece of text is separated into smaller chunks - in this case, words. Each unique word is assigned a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6017fd2f-a2a7-4be4-abf1-2cba80ae51e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words:  11753\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# tokenize the words in the headlines\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_headlines)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print('Total words: ', total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6e2eba-0b41-4dcb-9152-1bdf499d4332",
   "metadata": {},
   "source": [
    "Can use word_index dictionary to see how the tokenizer saves the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9c20aa4-18f1-466a-8c07-ddadc9af532b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 2, 'plan': 82, 'man': 139, 'panama': 2931, 'canal': 5487}\n"
     ]
    }
   ],
   "source": [
    "# print a subset of the word_index dictionary created by Tokenizer\n",
    "subset_dict = {key: value for key, value in tokenizer.word_index.items() \\\n",
    "                if key in ['a', 'man', 'a', 'plan', 'a', 'canal', 'panama']}\n",
    "print(subset_dict)  # print a subset to visualize the tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e03d25-d6e1-464d-a930-09cc91f9d5cc",
   "metadata": {},
   "source": [
    "Can use texts_to_sequences method to see how the tokenizer saves the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "159efd8a-639e-46de-b1da-777bbce15787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2], [139], [2], [82], [2], [5487], [2931]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['a', 'man', 'a', 'plan', 'a', 'canal', 'panama'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cd9d04-31d0-4189-984c-303080c56a03",
   "metadata": {},
   "source": [
    "### Creating Sequences\n",
    "Now that the data is tokenized, we will need to create a sequence of tokens from the headlines. These sequences will be used to train the deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fdd9303-d272-47cd-94a1-22b14ead1320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n f', 'n f l', 'n f l vs', 'n f l vs politics', 'n f l vs politics has']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[193, 125],\n",
       " [193, 125, 253],\n",
       " [193, 125, 253, 157],\n",
       " [193, 125, 253, 157, 226],\n",
       " [193, 125, 253, 157, 226, 83]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert data to sequence of tokens\n",
    "input_sequences = []\n",
    "for line in all_headlines:\n",
    "    # convert the headline into a sequence of tokens\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\n",
    "    # create a series of sequences for each headline\n",
    "    for i in range(1, len(token_list)):\n",
    "        partial_sequence = token_list[:i+1]\n",
    "        input_sequences.append(partial_sequence)\n",
    "\n",
    "print(tokenizer.sequences_to_texts(input_sequences[:5]))\n",
    "input_sequences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45167adf-ff2c-4c88-8aa8-1987d75bdd54",
   "metadata": {},
   "source": [
    "### Padding Sequences\n",
    "These sequences are of various lengths. For the model to be able to train on the data, all the sequences need to be the same length. This can be done by adding padding to the sequences with Keras pad_sequences method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "087fffed-83a2-4afd-8d69-6b927b179adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       193, 125], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# determine max sequence length\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "\n",
    "# pad all sequences with zeros at the beginning to make them all max length\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "input_sequences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4db32f-9a7c-47c9-a74e-dd11b27458a0",
   "metadata": {},
   "source": [
    "### Creating Predictors and Target\n",
    "We need to split up the sequences into predictors and a target. The last word of the sequence will be the target, and the first words of the sequence will be the predictors. For example, in \"nvidia releases ampere graphics cards\", the predictors are \"nvidia releases ampere graphics\" and the target/label is \"cards\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef1594eb-e059-452e-ab9c-c49901feeb1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([125, 253, 157, 226,  83], dtype=int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictors are every word except the last\n",
    "predictors = input_sequences[:,:-1]\n",
    "# labels are the last word\n",
    "labels = input_sequences[:,-1]\n",
    "labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7312533b-d624-48a5-9bd5-46398a26bade",
   "metadata": {},
   "source": [
    "The targets are categorical. We are predicting one word out of our possible total vocabulary. Instead of the model predicting scalar numbers, we will have it predict binary categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9169adad-7396-404d-a35d-a77cd1c775e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import utils\n",
    "\n",
    "labels = utils.to_categorical(labels, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be715c4c-d6af-4bde-b159-6e4a51d6d847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
